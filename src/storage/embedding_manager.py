"""
å¤šç§Embeddingæ–¹æ¡ˆç®¡ç†å™¨
æ”¯æŒå¤šç§å…è´¹çš„embeddingæ¨¡å‹ï¼Œæ— éœ€APIå¯†é’¥
"""

import logging
import hashlib
import numpy as np
from typing import List, Optional, Dict, Any, Union
from pathlib import Path

logger = logging.getLogger(__name__)

class EmbeddingManager:
    """å¤šæ–¹æ¡ˆEmbeddingç®¡ç†å™¨ - ä½¿ç”¨å•ä¾‹æ¨¡å¼"""
    
    _instance = None  # å•ä¾‹å®ä¾‹
    _initialized = False  # æ˜¯å¦å·²åˆå§‹åŒ–
    
    def __new__(cls, preferred_method: str = "all-MiniLM-L6-v2"):
        """å•ä¾‹æ¨¡å¼ï¼šç¡®ä¿åªåˆ›å»ºä¸€ä¸ªå®ä¾‹"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, preferred_method: str = "all-MiniLM-L6-v2"):
        """
        åˆå§‹åŒ–Embeddingç®¡ç†å™¨
        
        Args:
            preferred_method: é¦–é€‰embeddingæ–¹æ³•
            - "all-MiniLM-L6-v2": Sentence Transformers (å…è´¹)
            - "text-hash": æ–‡æœ¬å“ˆå¸Œ (æœ€ç®€å•)
            - "sentence-transformers-local": æœ¬åœ°Sentence Transformers
            - "huggingface-embeddings": Hugging Faceå…è´¹æ¨¡å‹
            - "bert-base-nli-mean-tokens": BERTåµŒå…¥
        """
        # å•ä¾‹æ¨¡å¼ï¼šåªåˆå§‹åŒ–ä¸€æ¬¡
        if EmbeddingManager._initialized:
            return
            
        self.preferred_method = preferred_method
        self.method = None
        self.model = None
        self.embedding_dim = 0
        
        # æ·»åŠ ç¼“å­˜æœºåˆ¶
        self.embedding_cache = {}  # ç”¨äºç¼“å­˜å·²è®¡ç®—çš„embedding
        self.cache_enabled = True  # ç¼“å­˜å¼€å…³
        self.max_cache_size = 1000  # æœ€å¤§ç¼“å­˜æ•°é‡
        
        self._initialize_embedding_method()
        EmbeddingManager._initialized = True
    
    def _initialize_embedding_method(self):
        """åˆå§‹åŒ–embeddingæ–¹æ³•"""
        try:
            # å°è¯•ä¸åŒçš„embeddingæ–¹æ³•
            methods_tried = []
            
            # æ–¹æ³•1: Sentence Transformers (å…è´¹ï¼Œæ¨è)
            if self._try_sentence_transformers():
                logger.info("âœ… ä½¿ç”¨ Sentence Transformers (all-MiniLM-L6-v2)")
                return
            
            methods_tried.append("Sentence Transformers")
            
            # æ–¹æ³•2: ç®€åŒ–æ–‡æœ¬å“ˆå¸Œ
            if self._try_text_hash():
                logger.info("âœ… ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œembedding")
                return
                
            methods_tried.append("æ–‡æœ¬å“ˆå¸Œ")
            
            # æ–¹æ³•3: è¯è¢‹æ¨¡å‹
            if self._try_bag_of_words():
                logger.info("âœ… ä½¿ç”¨è¯è¢‹æ¨¡å‹embedding")
                return
                
            methods_tried.append("è¯è¢‹æ¨¡å‹")
            
            logger.warning(f"æ‰€æœ‰embeddingæ–¹æ³•å°è¯•å¤±è´¥: {methods_tried}")
            logger.info("ğŸ”„ é»˜è®¤ä½¿ç”¨384ç»´æ–‡æœ¬å“ˆå¸Œæ–¹æ³•")
            self.method = "text-hash"
            self.embedding_dim = 384  # å‡çº§åˆ°384ç»´ï¼ŒåŒ¹é…Sentence Transformers
            
        except Exception as e:
            logger.error(f"Embeddingåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå·¥ä½œæ–¹æ³•
            self.method = "text-hash"
            self.embedding_dim = 384  # å‡çº§åˆ°384ç»´ï¼ŒåŒ¹é…Sentence Transformers
    
    def _try_sentence_transformers(self) -> bool:
        """å°è¯•Sentence Transformers"""
        try:
            import urllib.request
            import urllib.error
            
            # å¿«é€Ÿæ£€æŸ¥ç½‘ç»œè¿æ¥ - åªå°è¯•ä¸€æ¬¡
            try:
                urllib.request.urlopen("https://huggingface.co", timeout=5)
            except (urllib.error.URLError, OSError):
                logger.debug("ç½‘ç»œä¸å¯ç”¨ï¼Œè·³è¿‡Sentence Transformers")
                return False
            
            from sentence_transformers import SentenceTransformer
            
            # å°è¯•åŠ è½½å…è´¹æ¨¡å‹
            model_name = "all-MiniLM-L6-v2"  # Hugging Faceå…è´¹æ¨¡å‹
            self.model = SentenceTransformer(model_name)
            self.method = "sentence-transformers"
            self.embedding_dim = 384
            logger.info("âœ… æˆåŠŸåŠ è½½Sentence Transformersæ¨¡å‹")
            return True
            
        except Exception as e:
            logger.debug(f"Sentence Transformers ä¸å¯ç”¨: {e}")
            return False
    
    def _try_text_hash(self) -> bool:
        """ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œ - 384ç»´é«˜è´¨é‡ç‰ˆæœ¬"""
        self.method = "text-hash"
        self.embedding_dim = 384  # å‡çº§åˆ°384ç»´ï¼ŒåŒ¹é…Sentence Transformers
        return True
    
    def _try_bag_of_words(self) -> bool:
        """å°è¯•è¯è¢‹æ¨¡å‹"""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦sklearn
            from sklearn.feature_extraction.text import TfidfVectorizer
            self.method = "bow-tfidf"
            self.embedding_dim = 1000
            return True
            
        except Exception as e:
            logger.debug(f"BOWæ–¹æ³•ä¸å¯ç”¨: {e}")
            return False
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æ¡£embedding
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            embeddingå‘é‡åˆ—è¡¨
        """
        try:
            if self.method == "sentence-transformers":
                return self._embed_sentence_transformers(texts)
            elif self.method == "text-hash":
                return self._embed_text_hash(texts)
            elif self.method == "bow-tfidf":
                return self._embed_bow_tfidf(texts)
            else:
                # å›é€€åˆ°æ–‡æœ¬å“ˆå¸Œ
                return self._embed_text_hash(texts)
                
        except Exception as e:
            logger.error(f"æ–‡æ¡£embeddingç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._embed_text_hash(texts)
    
    def embed_query(self, text: str) -> List[float]:
        """
        ç”ŸæˆæŸ¥è¯¢æ–‡æœ¬embedding (å¸¦ç¼“å­˜)
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            embeddingå‘é‡
        """
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_enabled:
            cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
            
            if cache_key in self.embedding_cache:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„embedding: {text[:30]}...")
                return self.embedding_cache[cache_key]
        
        # ç”Ÿæˆembedding
        embeddings = self.embed_documents([text])
        result = embeddings[0] if embeddings else [0.0] * self.embedding_dim
        
        # ç¼“å­˜ç»“æœ
        if self.cache_enabled:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†æœ€æ—§çš„ä¸€åŠ
            if len(self.embedding_cache) >= self.max_cache_size:
                keys_to_remove = list(self.embedding_cache.keys())[:self.max_cache_size // 2]
                for key in keys_to_remove:
                    del self.embedding_cache[key]
                logger.debug(f"ç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†äº† {len(keys_to_remove)} ä¸ªæ—§æ¡ç›®")
            
            self.embedding_cache[cache_key] = result
            logger.debug(f"ç¼“å­˜äº†æ–°çš„embedding (ç¼“å­˜å¤§å°: {len(self.embedding_cache)})")
        
        return result
        return embeddings[0] if embeddings else [0.0] * self.embedding_dim
    
    def _embed_sentence_transformers(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨Sentence Transformersç”Ÿæˆembedding"""
        try:
            return self.model.encode(texts, show_progress_bar=False).tolist()
        except Exception as e:
            logger.error(f"Sentence Transformers embeddingå¤±è´¥: {e}")
            return self._embed_text_hash(texts)
    
    def _embed_text_hash(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œç”Ÿæˆembedding"""
        embeddings = []
        for text in texts:
            # ç”Ÿæˆå¤šä¸ªå“ˆå¸Œç‰¹å¾
            hash_features = []
            for i in range(self.embedding_dim):
                # ä½¿ç”¨ä¸åŒç§å­ç”Ÿæˆå“ˆå¸Œ
                hash_obj = hashlib.md5(f"{text}_{i}".encode())
                # è½¬æ¢ä¸º0-1ä¹‹é—´çš„æµ®ç‚¹æ•°
                hash_value = int(hash_obj.hexdigest(), 16) % 1000
                hash_features.append(hash_value / 1000.0)
            
            # ç¡®ä¿å‘é‡é•¿åº¦æ­£ç¡®
            if len(hash_features) != self.embedding_dim:
                hash_features = hash_features[:self.embedding_dim]
                while len(hash_features) < self.embedding_dim:
                    hash_features.append(hash_features[len(hash_features) % len(hash_features)])
            
            embeddings.append(hash_features)
        
        return embeddings
    
    def _embed_bow_tfidf(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨è¯è¢‹æ¨¡å‹ç”Ÿæˆembedding"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            
            # åˆ›å»ºæˆ–æ›´æ–°TF-IDFå‘é‡å™¨
            if not hasattr(self, '_tfidf_vectorizer'):
                self._tfidf_vectorizer = TfidfVectorizer(
                    max_features=self.embedding_dim,
                    stop_words='english',
                    lowercase=True
                )
                self._fitted = False
            
            if not self._fitted:
                tfidf_matrix = self._tfidf_vectorizer.fit_transform(texts)
                self._fitted = True
            else:
                tfidf_matrix = self._tfidf_vectorizer.transform(texts)
            
            # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µå¹¶æ ‡å‡†åŒ–
            embeddings = tfidf_matrix.toarray().tolist()
            return embeddings
            
        except Exception as e:
            logger.error(f"TF-IDF embeddingå¤±è´¥: {e}")
            return self._embed_text_hash(texts)
    
    def get_method_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰embeddingæ–¹æ³•ä¿¡æ¯"""
        return {
            "method": self.method,
            "dimension": self.embedding_dim,
            "description": self._get_method_description(),
            "is_free": True,
            "privacy_protected": True
        }
    
    def _get_method_description(self) -> str:
        """è·å–æ–¹æ³•æè¿°"""
        descriptions = {
            "sentence-transformers": "Sentence Transformers (all-MiniLM-L6-v2) - å…è´¹é«˜è´¨é‡è¯­ä¹‰embedding",
            "text-hash": "æ–‡æœ¬å“ˆå¸ŒåµŒå…¥ - ç®€å•å¿«é€Ÿï¼ŒåŸºäºå†…å®¹å“ˆå¸Œ",
            "bow-tfidf": "è¯è¢‹TF-IDFæ¨¡å‹ - åŸºäºè¯é¢‘çš„ç»Ÿè®¡embedding",
        }
        return descriptions.get(self.method, "æœªçŸ¥æ–¹æ³•")
    
    @staticmethod
    def get_available_methods() -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„embeddingæ–¹æ³•"""
        methods = [
            {
                "name": "Sentence Transformers",
                "model": "all-MiniLM-L6-v2",
                "dimension": 384,
                "quality": "é«˜",
                "speed": "å¿«",
                "is_free": True,
                "description": "Hugging Faceå…è´¹æ¨¡å‹ï¼Œè¯­ä¹‰ç†è§£èƒ½åŠ›å¼º",
                "install": "pip install sentence-transformers"
            },
            {
                "name": "æ–‡æœ¬å“ˆå¸Œ",
                "model": "custom",
                "dimension": 384,
                "quality": "ä¸­ç­‰",
                "speed": "æå¿«",
                "is_free": True,
                "description": "åŸºäºå†…å®¹å“ˆå¸Œï¼Œé›¶ä¾èµ–",
                "install": "æ— éœ€å®‰è£…"
            },
            {
                "name": "TF-IDFè¯è¢‹",
                "model": "sklearn",
                "dimension": 1000,
                "quality": "ä¸­ç­‰",
                "speed": "å¿«",
                "is_free": True,
                "description": "åŸºäºè¯é¢‘ç»Ÿè®¡ï¼Œé€‚åˆå…³é”®è¯æœç´¢",
                "install": "pip install scikit-learn"
            }
        ]
        return methods
    
    def clear_cache(self):
        """æ¸…ç©ºembeddingç¼“å­˜"""
        cache_size = len(self.embedding_cache)
        self.embedding_cache.clear()
        logger.info(f"å·²æ¸…ç©ºembeddingç¼“å­˜ (æ¸…ç†äº† {cache_size} ä¸ªæ¡ç›®)")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cache_enabled": self.cache_enabled,
            "cache_size": len(self.embedding_cache),
            "max_cache_size": self.max_cache_size,
            "cache_hit_rate": "N/A"  # å¯ä»¥åç»­æ·»åŠ å‘½ä¸­ç‡ç»Ÿè®¡
        }
    
    def set_cache_enabled(self, enabled: bool):
        """è®¾ç½®ç¼“å­˜å¼€å…³"""
        self.cache_enabled = enabled
        logger.info(f"Embeddingç¼“å­˜{'å·²å¯ç”¨' if enabled else 'å·²ç¦ç”¨'}")