"""
ç®€åŒ–ç‰ˆå‘é‡å­˜å‚¨æ¨¡å—
ä¸ä½¿ç”¨langchain_chromaï¼Œç›´æ¥ä½¿ç”¨chromadb
"""

import os
import logging
import time
import sqlite3
from typing import List, Optional, Dict, Any
from pathlib import Path
import numpy as np
import chromadb
from chromadb.config import Settings as ChromaSettings
from langchain_core.documents import Document as LangChainDocument
from config.settings import settings
from rank_bm25 import BM25Okapi

# æ•°æ®åº“é‡è¯•é…ç½®
DB_MAX_RETRIES = 5
DB_RETRY_DELAY = 0.5  # ç§’
DB_TIMEOUT = 10.0  # ç§’

# å°è¯•å¯¼å…¥embeddingç®¡ç†å™¨
try:
    from .embedding_manager import EmbeddingManager
    EMBEDDING_MANAGER_AVAILABLE = True
except ImportError as e:
    EMBEDDING_MANAGER_AVAILABLE = False
    logging.warning(f"Embeddingç®¡ç†å™¨ä¸å¯ç”¨: {e}")

# å°è¯•å¯¼å…¥OpenAIï¼ˆå¯é€‰ï¼‰
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logger = logging.getLogger(__name__)

class VectorStore:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, collection_name: str = "knowledge_base"):
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        self.embeddings = None
        
        self._initialize_store()
    
    def _initialize_store(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        try:
            # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
            os.makedirs(settings.VECTOR_DB_PATH, exist_ok=True)
            
            # åˆ›å»ºChromaå®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(
                path=settings.VECTOR_DB_PATH
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.client.get_collection(self.collection_name)
                logger.info(f"ä½¿ç”¨ç°æœ‰é›†åˆ: {self.collection_name}")
            except:
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "ä¸ªäººçŸ¥è¯†åº“å‘é‡å­˜å‚¨"}
                )
                logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
            
            # åˆå§‹åŒ–embeddingsç³»ç»Ÿï¼ˆä¼˜å…ˆçº§ï¼šOpenAI > å…è´¹æ–¹æ¡ˆï¼‰
            self._initialize_embeddings()
            
            logger.info(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ: {self.collection_name}")
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _initialize_embeddings(self):
        """åˆå§‹åŒ–embeddingç³»ç»Ÿ"""
        # ä¼˜å…ˆçº§1: OpenAIï¼ˆå¦‚æœæœ‰APIå¯†é’¥ä¸”å¯ç”¨ï¼‰
        if settings.OPENAI_API_KEY and OPENAI_AVAILABLE:
            try:
                self.embeddings = OpenAIEmbeddings(
                    model=settings.EMBEDDING_MODEL,
                    openai_api_key=settings.OPENAI_API_KEY
                )
                logger.info("âœ… ä½¿ç”¨OpenAI Embeddings")
                return
            except Exception as e:
                logger.warning(f"OpenAI Embeddingsåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: å…è´¹Embeddingç®¡ç†å™¨
        if EMBEDDING_MANAGER_AVAILABLE:
            try:
                # ä½¿ç”¨æ™ºèƒ½é€‰æ‹©æœ€ä½³embeddingæ–¹æ³•
                optimal_method = settings.get_optimal_embedding_method()
                self.embedding_manager = EmbeddingManager(optimal_method)
                self.embeddings = self.embedding_manager
                method_info = self.embedding_manager.get_method_info()
                logger.info(f"âœ… ä½¿ç”¨Embeddingæ–¹æ¡ˆ: {method_info['description']}")
                return
            except Exception as e:
                logger.warning(f"å…è´¹Embeddingç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        # é™çº§: ç®€å•æ–‡æœ¬å“ˆå¸Œ
        logger.info("ğŸ”„ ä½¿ç”¨ç®€å•æ–‡æœ¬å“ˆå¸Œembedding")
        self.embeddings = None

    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆembeddingå‘é‡"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨Embeddingç®¡ç†å™¨
            if hasattr(self, 'embedding_manager'):
                return self.embedding_manager.embed_documents(texts)

            # æ–¹æ³•2: ä½¿ç”¨OpenAI
            elif hasattr(self, 'embeddings') and self.embeddings is not None and not isinstance(self.embeddings, type(None)):
                return self.embeddings.embed_documents(texts)

            # é™çº§æ–¹æ³•3: ç®€å•æ–‡æœ¬å“ˆå¸Œ
            else:
                logger.debug("ä½¿ç”¨ç®€å•æ–‡æœ¬å“ˆå¸Œembedding")
                return [[hash(text) % 1000 for _ in range(3)] for text in texts]

        except Exception as e:
            logger.warning(f"embeddingç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬å“ˆå¸Œ: {e}")
            return [[hash(text) % 1000 for _ in range(3)] for text in texts]

    def _generate_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢embedding"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨Embeddingç®¡ç†å™¨
            if hasattr(self, 'embedding_manager'):
                return self.embedding_manager.embed_query(query)

            # æ–¹æ³•2: ä½¿ç”¨OpenAI
            elif hasattr(self, 'embeddings') and self.embeddings is not None and not isinstance(self.embeddings, type(None)):
                return self.embeddings.embed_query(query)

            # é™çº§æ–¹æ³•3: ç®€å•æ–‡æœ¬å“ˆå¸Œ
            else:
                logger.debug("ä½¿ç”¨ç®€å•æ–‡æœ¬å“ˆå¸ŒæŸ¥è¯¢embedding")
                return [hash(query) % 1000 for _ in range(3)]

        except Exception as e:
            logger.warning(f"æŸ¥è¯¢embeddingç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬å“ˆå¸Œ: {e}")
            return [hash(query) % 1000 for _ in range(3)]

    def add_documents(self, documents: List[LangChainDocument]) -> bool:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨ï¼ˆå¸¦æ•°æ®åº“é”é‡è¯•ï¼‰

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        retries = 0
        last_error = None
        
        while retries < DB_MAX_RETRIES:
            try:
                if not documents:
                    logger.warning("æ²¡æœ‰æ–‡æ¡£éœ€è¦æ·»åŠ ")
                    return False

                # ç”Ÿæˆembedding
                texts = [doc.page_content for doc in documents]
                metadatas = [doc.metadata for doc in documents]
                ids = [f"doc_{hash(doc.page_content)}_{i}" for i, doc in enumerate(documents)]

                embeddings = self._generate_embeddings(texts)

                # æ·»åŠ åˆ°é›†åˆ
                self.collection.add(
                    documents=texts,
                    metadatas=metadatas,
                    embeddings=embeddings,
                    ids=ids
                )

                logger.info(f"æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£å—")
                return True

            except sqlite3.OperationalError as e:
                last_error = e
                error_msg = str(e).lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“é”å®šé”™è¯¯
                if 'locked' in error_msg or 'database is locked' in error_msg:
                    retries += 1
                    if retries < DB_MAX_RETRIES:
                        delay = DB_RETRY_DELAY * retries
                        logger.warning(f"æ•°æ®åº“é”å®šï¼Œ{delay}ç§’åé‡è¯• ({retries}/{DB_MAX_RETRIES})")
                        time.sleep(delay)
                        continue
                    else:
                        logger.error(f"æ•°æ®åº“é”å®šé‡è¯•å¤±è´¥: {str(e)}")
                        break
                else:
                    # å…¶ä»–æ•°æ®åº“é”™è¯¯ï¼Œä¸é‡è¯•
                    logger.error(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {str(e)}")
                    break
                    
            except Exception as e:
                last_error = e
                logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
                break
        
        logger.error(f"æ·»åŠ æ–‡æ¡£æœ€ç»ˆå¤±è´¥: {str(last_error)}")
        return False

    def search(self, query: str, k: int = 5, filter_dict: Optional[Dict] = None) -> List[LangChainDocument]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£ï¼ˆå¸¦æ•°æ®åº“é”é‡è¯•ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ–‡æ¡£æ•°é‡
            filter_dict: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
        """
        retries = 0
        last_error = None
        
        while retries < DB_MAX_RETRIES:
            try:
                # ç”ŸæˆæŸ¥è¯¢embedding
                query_embedding = self._generate_query_embedding(query)

                # æœç´¢
                where_clause = None
                if filter_dict:
                    where_clause = {}
                    for key, value in filter_dict.items():
                        where_clause[key] = {"$eq": value}

                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=min(k, settings.TOP_K * 2),
                    where=where_clause
                )

                # è½¬æ¢ä¸ºLangChainæ–‡æ¡£æ ¼å¼
                documents = []
                if results['documents'] and results['documents'][0]:
                    # è·å–æ‰€æœ‰è·ç¦»å€¼ç”¨äºå½’ä¸€åŒ–
                    all_distances = results['distances'][0] if results['distances'] else [0.0] * len(results['documents'][0])
                    min_distance = min(all_distances) if all_distances else 0.0
                    max_distance = max(all_distances) if all_distances else 1.0

                    for i, (doc_content, metadata, doc_id) in enumerate(zip(
                        results['documents'][0],
                        results['metadatas'][0] if results['metadatas'] else [],
                        results['ids'][0] if results['ids'] else []
                    )):
                        # æ”¹è¿›çš„ç›¸ä¼¼åº¦åˆ†æ•°è®¡ç®—
                        distance = all_distances[i] if i < len(all_distances) else 0.0

                        # åŠ¨æ€å½’ä¸€åŒ–ï¼šä½¿ç”¨ç›¸å¯¹è·ç¦»è®¡ç®—ç›¸å…³æ€§
                        if max_distance > min_distance:
                            # ç›¸å¯¹è·ç¦»å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
                            relative_distance = (distance - min_distance) / (max_distance - min_distance)
                            relevance_score = 1.0 - relative_distance
                        else:
                            # æ‰€æœ‰è·ç¦»ç›¸åŒï¼Œè®¾ä¸ºä¸­ç­‰ç›¸å…³æ€§
                            relevance_score = 0.5

                        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…ï¼Œé¿å…è¿‡åº¦ä¹è§‚
                        if distance > 2.0:  # å¯¹äºè·ç¦»å¾ˆè¿œçš„æ–‡æ¡£
                            relevance_score = max(0.0, min(0.3, relevance_score))
                        elif distance > 1.5:  # è·ç¦»è¾ƒè¿œçš„æ–‡æ¡£
                            relevance_score = max(0.1, min(0.5, relevance_score))
                        elif distance < 0.3:  # éå¸¸ç›¸ä¼¼çš„æ–‡æ¡£
                            relevance_score = max(0.7, min(1.0, relevance_score))
                        else:  # ä¸­ç­‰è·ç¦»
                            relevance_score = max(0.2, min(0.8, relevance_score))

                        doc = LangChainDocument(
                            page_content=doc_content,
                            metadata={
                                **metadata,
                                "search_score": distance,
                                "relevance_score": round(relevance_score, 3),  # ä¿ç•™3ä½å°æ•°
                                "doc_id": doc_id
                            }
                        )
                        documents.append(doc)

                # å¦‚æœç»“æœå°‘äºéœ€è¦çš„æ•°é‡ï¼Œå°è¯•æ›´å¤šç»“æœ
                if len(documents) < k and self.collection.count() > 0:
                    logger.info(f"æœç´¢ç»“æœä¸è¶³ï¼Œå°è¯•è·å–æ›´å¤šç»“æœ")
                    more_results = self.collection.query(
                        query_embeddings=[query_embedding],
                        n_results=min(self.collection.count(), k * 3),
                        where=where_clause
                    )

                    # é‡æ–°å¤„ç†ç»“æœ
                    if more_results['documents'] and more_results['documents'][0]:
                        additional_docs = []
                        all_more_distances = more_results['distances'][0] if more_results['distances'] else [0.0] * len(more_results['documents'][0])
                        min_more_distance = min(all_more_distances) if all_more_distances else 0.0
                        max_more_distance = max(all_more_distances) if all_more_distances else 1.0

                        for i, (doc_content, metadata, doc_id) in enumerate(zip(
                            more_results['documents'][0],
                            more_results['metadatas'][0] if more_results['metadatas'] else [],
                            more_results['ids'][0] if more_results['ids'] else []
                        )):
                            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨
                            if not any(doc.page_content == doc_content for doc in documents):
                                distance = all_more_distances[i] if i < len(all_more_distances) else 0.0

                                # ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–é€»è¾‘
                                if max_more_distance > min_more_distance:
                                    relative_distance = (distance - min_more_distance) / (max_more_distance - min_more_distance)
                                    relevance_score = 1.0 - relative_distance
                                else:
                                    relevance_score = 0.5

                                if distance > 2.0:
                                    relevance_score = max(0.0, min(0.3, relevance_score))
                                elif distance > 1.5:
                                    relevance_score = max(0.1, min(0.5, relevance_score))
                                elif distance < 0.3:
                                    relevance_score = max(0.7, min(1.0, relevance_score))
                                else:
                                    relevance_score = max(0.2, min(0.8, relevance_score))

                                doc = LangChainDocument(
                                    page_content=doc_content,
                                    metadata={
                                        **metadata,
                                        "search_score": distance,
                                        "relevance_score": round(relevance_score, 3),
                                        "doc_id": doc_id
                                    }
                                )
                                additional_docs.append(doc)
                        
                        documents.extend(additional_docs)
                
                logger.info(f"æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:50]}...', ç»“æœæ•°é‡={len(documents)}")
                return documents[:k]
                
            except sqlite3.OperationalError as e:
                last_error = e
                error_msg = str(e).lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“é”å®šé”™è¯¯
                if 'locked' in error_msg or 'database is locked' in error_msg:
                    retries += 1
                    if retries < DB_MAX_RETRIES:
                        delay = DB_RETRY_DELAY * retries
                        logger.warning(f"æ•°æ®åº“é”å®šï¼Œ{delay}ç§’åé‡è¯• ({retries}/{DB_MAX_RETRIES})")
                        time.sleep(delay)
                        continue
                    else:
                        logger.error(f"æ•°æ®åº“é”å®šé‡è¯•å¤±è´¥: {str(e)}")
                        break
                else:
                    logger.error(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {str(e)}")
                    break
                    
            except Exception as e:
                last_error = e
                logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
                break
        
        logger.error(f"æœç´¢æœ€ç»ˆå¤±è´¥: {str(last_error)}")
        return []
    
    def delete_documents(self, filter_dict: Dict[str, Any]) -> bool:
        """
        åˆ é™¤æ–‡æ¡£
        
        Args:
            filter_dict: åˆ é™¤æ¡ä»¶ï¼ˆå¦‚ {"filename": "test.md"}ï¼‰
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = {}
            for key, value in filter_dict.items():
                where_clause[key] = {"$eq": value}
            
            # å…ˆæŸ¥è¯¢è¦åˆ é™¤çš„æ–‡æ¡£æ•°é‡
            try:
                existing_docs = self.collection.get(where=where_clause)
                doc_count = len(existing_docs['ids']) if existing_docs and 'ids' in existing_docs else 0
                logger.info(f"æ‰¾åˆ° {doc_count} ä¸ªåŒ¹é…çš„æ–‡æ¡£å—å‡†å¤‡åˆ é™¤")
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢å¾…åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
                doc_count = 0
            
            # æ‰§è¡Œåˆ é™¤
            if doc_count > 0:
                self.collection.delete(where=where_clause)
                logger.info(f"åˆ é™¤æ–‡æ¡£æˆåŠŸ: {filter_dict}, åˆ é™¤äº† {doc_count} ä¸ªæ–‡æ¡£å—")
            else:
                logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£: {filter_dict}")
            
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            stats = {
                "collection_name": self.collection_name,
                "total_documents": self.collection.count(),
                "vector_db_path": settings.VECTOR_DB_PATH
            }
            
            # æ·»åŠ embeddingä¿¡æ¯
            if hasattr(self, 'embedding_manager'):
                method_info = self.embedding_manager.get_method_info()
                stats.update({
                    "embedding_method": method_info["method"],
                    "embeddings_dimension": method_info["dimension"],
                    "embedding_description": method_info["description"],
                    "is_free_embedding": method_info["is_free"],
                    "privacy_protected": method_info["privacy_protected"]
                })
            elif hasattr(self, 'embeddings') and self.embeddings is not None:
                stats.update({
                    "embedding_method": "openai",
                    "embeddings_dimension": 1536,
                    "embedding_description": "OpenAI Embeddings",
                    "is_free_embedding": False,
                    "privacy_protected": False
                })
            else:
                stats.update({
                    "embedding_method": "text-hash",
                    "embeddings_dimension": 384,
                    "embedding_description": "ç®€å•æ–‡æœ¬å“ˆå¸Œ",
                    "is_free_embedding": True,
                    "privacy_protected": True
                })
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def reset(self) -> bool:
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        try:
            self.client.delete_collection(self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "ä¸ªäººçŸ¥è¯†åº“å‘é‡å­˜å‚¨"}
            )
            logger.info("å‘é‡å­˜å‚¨é‡ç½®æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨é‡ç½®å¤±è´¥: {str(e)}")
            return False

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.bm25_cache = {}
    
    def hybrid_search(self, query: str, k: int = 5, 
                     vector_weight: float = 0.7, keyword_weight: float = 0.3,
                     filter_dict: Optional[Dict] = None) -> List[LangChainDocument]:
        """
        æ··åˆæ£€ç´¢ï¼šå‘é‡æœç´¢ + å…³é”®è¯æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ–‡æ¡£æ•°é‡
            vector_weight: å‘é‡æœç´¢æƒé‡
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡
            filter_dict: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æ··åˆæ£€ç´¢ç»“æœ
        """
        try:
            # å‘é‡è¯­ä¹‰æœç´¢
            vector_results = self.vector_store.search(
                query, 
                k=k*2, 
                filter_dict=filter_dict
            )
            
            # ç®€å•å…³é”®è¯æœç´¢ï¼ˆåŸºäºæ–‡ä»¶åå’Œå†…å®¹ï¼‰
            keyword_results = self._keyword_search(
                query, 
                k=k*2, 
                filter_dict=filter_dict
            )
            
            # èåˆç»“æœ
            combined_results = self._fusion_results(
                query,
                vector_results,
                keyword_results,
                vector_weight,
                keyword_weight
            )
            
            logger.info(f"æ··åˆæ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query[:30]}...', ç»“æœæ•°é‡={len(combined_results)}")
            return combined_results[:k]
            
        except Exception as e:
            logger.error(f"æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}")
            # é™çº§åˆ°å‘é‡æœç´¢
            return self.vector_store.search(query, k=k, filter_dict=filter_dict)
    
    def _keyword_search(self, query: str, k: int, filter_dict: Optional[Dict]) -> List[LangChainDocument]:
        """BM25å…³é”®è¯æœç´¢ï¼ˆå¢å¼ºæ–‡ä»¶ååŒ¹é…ï¼‰"""
        try:
            # ç©ºæŸ¥è¯¢è¿”å›ç©ºç»“æœ
            if not query or not query.strip():
                return []
            
            # 1. è·å–æ‰€æœ‰æ–‡æ¡£
            all_docs = self.vector_store.collection.get()

            if not all_docs['documents']:
                return []

            # 2. åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
            filtered_docs = []
            filtered_metadatas = []
            filtered_ids = []

            for i, (doc, metadata, doc_id) in enumerate(zip(
                all_docs['documents'],
                all_docs['metadatas'],
                all_docs['ids']
            )):
                # å…ƒæ•°æ®è¿‡æ»¤
                if filter_dict:
                    match = all(
                        metadata.get(key) == value
                        for key, value in filter_dict.items()
                    )
                    if not match:
                        continue

                filtered_docs.append(doc)
                filtered_metadatas.append(metadata)
                filtered_ids.append(doc_id)
            
            if not filtered_docs:
                return []

            # 3. æ„å»ºBM25ç´¢å¼•ï¼ˆåŒ…å«æ–‡ä»¶åä¿¡æ¯ä»¥æé«˜åŒ¹é…ï¼‰
            tokenized_docs = []
            for doc, metadata in zip(filtered_docs, filtered_metadatas):
                # å°†æ–‡ä»¶åä¹ŸåŠ å…¥æœç´¢å†…å®¹ï¼Œæé«˜æ–‡ä»¶ååŒ¹é…æƒé‡
                filename = metadata.get('filename', '')
                # æ–‡ä»¶åé‡å¤3æ¬¡ä»¥æé«˜å…¶æƒé‡
                enhanced_content = f"{filename} {filename} {filename} {doc}"
                tokenized_docs.append(enhanced_content.lower().split())
            
            bm25 = BM25Okapi(tokenized_docs)

            # 4. æœç´¢
            query_tokens = query.lower().split()
            scores = bm25.get_scores(query_tokens)
            
            # 5. æ–‡ä»¶åç²¾ç¡®åŒ¹é…åŠ åˆ†
            query_lower = query.lower()
            for idx, metadata in enumerate(filtered_metadatas):
                filename = metadata.get('filename', '').lower()
                # å¦‚æœæŸ¥è¯¢è¯åœ¨æ–‡ä»¶åä¸­ï¼Œå¤§å¹…æå‡åˆ†æ•°
                if query_lower in filename:
                    scores[idx] *= 3.0  # æ–‡ä»¶ååŒ¹é…3å€åŠ åˆ†
                # å¦‚æœæ–‡ä»¶ååŒ…å«æŸ¥è¯¢è¯çš„ä»»ä¸€token
                elif any(token in filename for token in query_tokens):
                    scores[idx] *= 2.0  # éƒ¨åˆ†åŒ¹é…2å€åŠ åˆ†

            # 6. è¿”å›Top-K
            top_indices = np.argsort(scores)[::-1][:k]

            # 7. æ„å»ºç»“æœ
            results = []
            for idx in top_indices:
                doc = LangChainDocument(
                    page_content=filtered_docs[idx],
                    metadata={
                        **filtered_metadatas[idx],
                        "bm25_score": float(scores[idx]),
                        "keyword_relevance": min(scores[idx] / 10.0, 1.0),
                        "doc_id": filtered_ids[idx]
                    }
                )
                results.append(doc)

            logger.info(f"BM25å…³é”®è¯æœç´¢å®Œæˆ: æŸ¥è¯¢='{query}', ç»“æœæ•°é‡={len(results)}")
            logger.debug(f"BM25å¾—åˆ†: {[r.metadata['bm25_score'] for r in results]}")
            return results
        except Exception as e:
            logger.error(f"BM25æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _fusion_results(self, query: str, vector_results: List[LangChainDocument], 
                       keyword_results: List[LangChainDocument], 
                       vector_weight: float, keyword_weight: float) -> List[LangChainDocument]:
        """èåˆæœç´¢ç»“æœï¼ˆæ”¹è¿›ç®—æ³•ï¼‰"""
        # ä½¿ç”¨å­—å…¸æ¥åˆå¹¶ç›¸åŒæ–‡æ¡£çš„åˆ†æ•°
        doc_scores = {}
        
        # å¤„ç†å‘é‡æœç´¢ç»“æœ
        for doc in vector_results:
            doc_hash = hash(doc.page_content)
            vector_score = doc.metadata.get('relevance_score', 0.5)
            
            if doc_hash not in doc_scores:
                doc_scores[doc_hash] = {
                    'doc': doc,
                    'vector_score': vector_score,
                    'keyword_score': 0.0
                }
            else:
                doc_scores[doc_hash]['vector_score'] = max(
                    doc_scores[doc_hash]['vector_score'], 
                    vector_score
                )
        
        # å¤„ç†å…³é”®è¯æœç´¢ç»“æœï¼ˆä½¿ç”¨çœŸå®çš„BM25åˆ†æ•°ï¼‰
        for doc in keyword_results:
            doc_hash = hash(doc.page_content)
            # æ”¹è¿›BM25åˆ†æ•°å½’ä¸€åŒ–
            bm25_score = doc.metadata.get('bm25_score', 0)
            
            # åŠ¨æ€å½’ä¸€åŒ–ï¼šå¦‚æœåˆ†æ•°å¾ˆé«˜ï¼Œè¯´æ˜åŒ¹é…åº¦å¥½
            if bm25_score > 10:
                keyword_score = min(bm25_score / 15.0, 1.0)  # é«˜åˆ†æƒ…å†µ
            elif bm25_score > 5:
                keyword_score = min(bm25_score / 10.0, 1.0)  # ä¸­ç­‰åˆ†æ•°
            elif bm25_score > 0:
                keyword_score = min(bm25_score / 5.0, 0.8)   # ä½åˆ†æƒ…å†µ
            else:
                keyword_score = 0.0
            
            # æ–‡ä»¶åç²¾ç¡®åŒ¹é…æ—¶å¤§å¹…æå‡åˆ†æ•°
            filename = doc.metadata.get('filename', '').lower()
            query_lower = query.lower()
            
            if query_lower in filename:
                # æ–‡ä»¶ååŒ…å«æŸ¥è¯¢è¯ï¼Œç»™äºˆé«˜åˆ†
                keyword_score = max(keyword_score * 2.0, 0.85)
                keyword_score = min(keyword_score, 1.0)
            elif any(token in filename for token in query_lower.split()):
                # æ–‡ä»¶ååŒ…å«éƒ¨åˆ†æŸ¥è¯¢è¯
                keyword_score = max(keyword_score * 1.5, 0.7)
                keyword_score = min(keyword_score, 0.95)
            
            if doc_hash not in doc_scores:
                doc_scores[doc_hash] = {
                    'doc': doc,
                    'vector_score': 0.0,
                    'keyword_score': keyword_score
                }
            else:
                doc_scores[doc_hash]['keyword_score'] = max(
                    doc_scores[doc_hash]['keyword_score'],
                    keyword_score
                )
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†å¹¶æ„å»ºç»“æœåˆ—è¡¨
        results = []
        for doc_hash, info in doc_scores.items():
            doc = info['doc']
            
            # åŠ¨æ€è°ƒæ•´æƒé‡ç­–ç•¥
            actual_keyword_weight = keyword_weight
            actual_vector_weight = vector_weight
            
            # ç­–ç•¥1: æ–‡ä»¶åç²¾ç¡®åŒ¹é… + é«˜å…³é”®è¯å¾—åˆ† â†’ æé«˜æƒé‡
            if info['keyword_score'] > 0.8 and info['vector_score'] < 0.3:
                # å…³é”®è¯åŒ¹é…å¾ˆå¥½ä½†å‘é‡åŒ¹é…å·®ï¼ˆæŠ€æœ¯æ–‡æ¡£åœºæ™¯ï¼‰
                # ç»™äºˆå…³é”®è¯æœç´¢ä¸»å¯¼æƒ
                actual_keyword_weight = 0.9
                actual_vector_weight = 0.1
            elif info['keyword_score'] > 0.8:
                # æ–‡ä»¶ååŒ¹é…åº¦é«˜æ—¶ï¼Œæå‡å…³é”®è¯æƒé‡
                actual_keyword_weight = min(keyword_weight * 1.5, 0.5)
                actual_vector_weight = 1.0 - actual_keyword_weight
            elif info['vector_score'] > 0.8 and info['keyword_score'] < 0.1:
                # å‘é‡åŒ¹é…å¾ˆå¥½ä½†å…³é”®è¯ä¸åŒ¹é…ï¼ˆè¯­ä¹‰ç›¸å…³ä½†æ–‡ä»¶åæ— å…³ï¼‰
                actual_vector_weight = 0.8
                actual_keyword_weight = 0.2
            
            combined_score = (
                actual_vector_weight * info['vector_score'] + 
                actual_keyword_weight * info['keyword_score']
            )
            
            # æ›´æ–°metadata
            doc.metadata['vector_score'] = round(info['vector_score'], 3)
            doc.metadata['keyword_score'] = round(info['keyword_score'], 3)
            doc.metadata['combined_score'] = round(combined_score, 3)
            doc.metadata['relevance_score'] = round(combined_score, 3)  # æ›´æ–°æ€»ç›¸å…³æ€§
            
            results.append(doc)
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        results.sort(key=lambda x: x.metadata.get('combined_score', 0), reverse=True)
        
        return results
