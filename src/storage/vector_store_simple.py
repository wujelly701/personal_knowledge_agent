"""
ç®€åŒ–ç‰ˆå‘é‡å­˜å‚¨æ¨¡å—
ä¸ä½¿ç”¨langchain_chromaï¼Œç›´æ¥ä½¿ç”¨chromadb
"""

import os
import logging
from typing import List, Optional, Dict, Any
from pathlib import Path
import numpy as np
import chromadb
from chromadb.config import Settings as ChromaSettings
from langchain_core.documents import Document as LangChainDocument
from config.settings import settings

# å°è¯•å¯¼å…¥embeddingç®¡ç†å™¨
try:
    from .embedding_manager import EmbeddingManager
    EMBEDDING_MANAGER_AVAILABLE = True
except ImportError as e:
    EMBEDDING_MANAGER_AVAILABLE = False
    logging.warning(f"Embeddingç®¡ç†å™¨ä¸å¯ç”¨: {e}")

# å°è¯•å¯¼å…¥OpenAIï¼ˆå¯é€‰ï¼‰
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logger = logging.getLogger(__name__)

class VectorStore:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, collection_name: str = "knowledge_base"):
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        self.embeddings = None
        
        self._initialize_store()
    
    def _initialize_store(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        try:
            # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
            os.makedirs(settings.VECTOR_DB_PATH, exist_ok=True)
            
            # åˆ›å»ºChromaå®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(
                path=settings.VECTOR_DB_PATH
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.client.get_collection(self.collection_name)
                logger.info(f"ä½¿ç”¨ç°æœ‰é›†åˆ: {self.collection_name}")
            except:
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "ä¸ªäººçŸ¥è¯†åº“å‘é‡å­˜å‚¨"}
                )
                logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
            
            # åˆå§‹åŒ–embeddingsç³»ç»Ÿï¼ˆä¼˜å…ˆçº§ï¼šOpenAI > å…è´¹æ–¹æ¡ˆï¼‰
            self._initialize_embeddings()
            
            logger.info(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ: {self.collection_name}")
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _initialize_embeddings(self):
        """åˆå§‹åŒ–embeddingç³»ç»Ÿ"""
        # ä¼˜å…ˆçº§1: OpenAIï¼ˆå¦‚æœæœ‰APIå¯†é’¥ä¸”å¯ç”¨ï¼‰
        if settings.OPENAI_API_KEY and OPENAI_AVAILABLE:
            try:
                self.embeddings = OpenAIEmbeddings(
                    model=settings.EMBEDDING_MODEL,
                    openai_api_key=settings.OPENAI_API_KEY
                )
                logger.info("âœ… ä½¿ç”¨OpenAI Embeddings")
                return
            except Exception as e:
                logger.warning(f"OpenAI Embeddingsåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: å…è´¹Embeddingç®¡ç†å™¨
        if EMBEDDING_MANAGER_AVAILABLE:
            try:
                # ä½¿ç”¨æ™ºèƒ½é€‰æ‹©æœ€ä½³embeddingæ–¹æ³•
                optimal_method = settings.get_optimal_embedding_method()
                self.embedding_manager = EmbeddingManager(optimal_method)
                self.embeddings = self.embedding_manager
                method_info = self.embedding_manager.get_method_info()
                logger.info(f"âœ… ä½¿ç”¨{'' if 'openai' not in method_info['method'] else 'å…è´¹'}Embeddingæ–¹æ¡ˆ: {method_info['description']}")
                return
            except Exception as e:
                logger.warning(f"å…è´¹Embeddingç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        # é™çº§: ç®€å•æ–‡æœ¬å“ˆå¸Œ
        logger.info("ğŸ”„ ä½¿ç”¨ç®€å•æ–‡æœ¬å“ˆå¸Œembedding")
        self.embeddings = None

    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆembeddingå‘é‡"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨Embeddingç®¡ç†å™¨
            if hasattr(self, 'embedding_manager'):
                return self.embedding_manager.embed_documents(texts)

            # æ–¹æ³•2: ä½¿ç”¨OpenAI
            elif hasattr(self, 'embeddings') and self.embeddings is not None and not isinstance(self.embeddings, type(None)):
                return self.embeddings.embed_documents(texts)

            # é™çº§æ–¹æ³•3: ç®€å•æ–‡æœ¬å“ˆå¸Œ
            else:
                logger.debug("ä½¿ç”¨ç®€å•æ–‡æœ¬å“ˆå¸Œembedding")
                return [[hash(text) % 1000 for _ in range(3)] for text in texts]

        except Exception as e:
            logger.warning(f"embeddingç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬å“ˆå¸Œ: {e}")
            return [[hash(text) % 1000 for _ in range(3)] for text in texts]

    def _generate_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢embedding"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨Embeddingç®¡ç†å™¨
            if hasattr(self, 'embedding_manager'):
                return self.embedding_manager.embed_query(query)

            # æ–¹æ³•2: ä½¿ç”¨OpenAI
            elif hasattr(self, 'embeddings') and self.embeddings is not None and not isinstance(self.embeddings, type(None)):
                return self.embeddings.embed_query(query)

            # é™çº§æ–¹æ³•3: ç®€å•æ–‡æœ¬å“ˆå¸Œ
            else:
                logger.debug("ä½¿ç”¨ç®€å•æ–‡æœ¬å“ˆå¸ŒæŸ¥è¯¢embedding")
                return [hash(query) % 1000 for _ in range(3)]

        except Exception as e:
            logger.warning(f"æŸ¥è¯¢embeddingç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬å“ˆå¸Œ: {e}")
            return [hash(query) % 1000 for _ in range(3)]

    def add_documents(self, documents: List[LangChainDocument]) -> bool:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            if not documents:
                logger.warning("æ²¡æœ‰æ–‡æ¡£éœ€è¦æ·»åŠ ")
                return False

            # ç”Ÿæˆembedding
            texts = [doc.page_content for doc in documents]
            metadatas = [doc.metadata for doc in documents]
            ids = [f"doc_{hash(doc.page_content)}_{i}" for i, doc in enumerate(documents)]

            embeddings = self._generate_embeddings(texts)

            # æ·»åŠ åˆ°é›†åˆ
            self.collection.add(
                documents=texts,
                metadatas=metadatas,
                embeddings=embeddings,
                ids=ids
            )

            logger.info(f"æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£å—")
            return True

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def search(self, query: str, k: int = 5, filter_dict: Optional[Dict] = None) -> List[LangChainDocument]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£

        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ–‡æ¡£æ•°é‡
            filter_dict: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢embedding
            query_embedding = self._generate_query_embedding(query)

            # æœç´¢
            where_clause = None
            if filter_dict:
                where_clause = {}
                for key, value in filter_dict.items():
                    where_clause[key] = {"$eq": value}

            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(k, settings.TOP_K * 2),
                where=where_clause
            )

            # è½¬æ¢ä¸ºLangChainæ–‡æ¡£æ ¼å¼
            documents = []
            if results['documents'] and results['documents'][0]:
                # è·å–æ‰€æœ‰è·ç¦»å€¼ç”¨äºå½’ä¸€åŒ–
                all_distances = results['distances'][0] if results['distances'] else [0.0] * len(results['documents'][0])
                min_distance = min(all_distances) if all_distances else 0.0
                max_distance = max(all_distances) if all_distances else 1.0

                for i, (doc_content, metadata, doc_id) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0] if results['metadatas'] else [],
                    results['ids'][0] if results['ids'] else []
                )):
                    # æ”¹è¿›çš„ç›¸ä¼¼åº¦åˆ†æ•°è®¡ç®—
                    distance = all_distances[i] if i < len(all_distances) else 0.0

                    # åŠ¨æ€å½’ä¸€åŒ–ï¼šä½¿ç”¨ç›¸å¯¹è·ç¦»è®¡ç®—ç›¸å…³æ€§
                    if max_distance > min_distance:
                        # ç›¸å¯¹è·ç¦»å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
                        relative_distance = (distance - min_distance) / (max_distance - min_distance)
                        relevance_score = 1.0 - relative_distance
                    else:
                        # æ‰€æœ‰è·ç¦»ç›¸åŒï¼Œè®¾ä¸ºä¸­ç­‰ç›¸å…³æ€§
                        relevance_score = 0.5

                    # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…ï¼Œé¿å…è¿‡åº¦ä¹è§‚
                    if distance > 2.0:  # å¯¹äºè·ç¦»å¾ˆè¿œçš„æ–‡æ¡£
                        relevance_score = max(0.0, min(0.3, relevance_score))
                    elif distance > 1.5:  # è·ç¦»è¾ƒè¿œçš„æ–‡æ¡£
                        relevance_score = max(0.1, min(0.5, relevance_score))
                    elif distance < 0.3:  # éå¸¸ç›¸ä¼¼çš„æ–‡æ¡£
                        relevance_score = max(0.7, min(1.0, relevance_score))
                    else:  # ä¸­ç­‰è·ç¦»
                        relevance_score = max(0.2, min(0.8, relevance_score))

                    doc = LangChainDocument(
                        page_content=doc_content,
                        metadata={
                            **metadata,
                            "search_score": distance,
                            "relevance_score": round(relevance_score, 3),  # ä¿ç•™3ä½å°æ•°
                            "doc_id": doc_id
                        }
                    )
                    documents.append(doc)

            # å¦‚æœç»“æœå°‘äºéœ€è¦çš„æ•°é‡ï¼Œå°è¯•æ›´å¤šç»“æœ
            if len(documents) < k and self.collection.count() > 0:
                logger.info(f"æœç´¢ç»“æœä¸è¶³ï¼Œå°è¯•è·å–æ›´å¤šç»“æœ")
                more_results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=min(self.collection.count(), k * 3),
                    where=where_clause
                )

                # é‡æ–°å¤„ç†ç»“æœ
                if more_results['documents'] and more_results['documents'][0]:
                    additional_docs = []
                    all_more_distances = more_results['distances'][0] if more_results['distances'] else [0.0] * len(more_results['documents'][0])
                    min_more_distance = min(all_more_distances) if all_more_distances else 0.0
                    max_more_distance = max(all_more_distances) if all_more_distances else 1.0

                    for i, (doc_content, metadata, doc_id) in enumerate(zip(
                        more_results['documents'][0],
                        more_results['metadatas'][0] if more_results['metadatas'] else [],
                        more_results['ids'][0] if more_results['ids'] else []
                    )):
                        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨
                        if not any(doc.page_content == doc_content for doc in documents):
                            distance = all_more_distances[i] if i < len(all_more_distances) else 0.0

                            # ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–é€»è¾‘
                            if max_more_distance > min_more_distance:
                                relative_distance = (distance - min_more_distance) / (max_more_distance - min_more_distance)
                                relevance_score = 1.0 - relative_distance
                            else:
                                relevance_score = 0.5

                            if distance > 2.0:
                                relevance_score = max(0.0, min(0.3, relevance_score))
                            elif distance > 1.5:
                                relevance_score = max(0.1, min(0.5, relevance_score))
                            elif distance < 0.3:
                                relevance_score = max(0.7, min(1.0, relevance_score))
                            else:
                                relevance_score = max(0.2, min(0.8, relevance_score))

                            doc = LangChainDocument(
                                page_content=doc_content,
                                metadata={
                                    **metadata,
                                    "search_score": distance,
                                    "relevance_score": round(relevance_score, 3),
                                    "doc_id": doc_id
                                }
                            )
                            additional_docs.append(doc)
                    
                    documents.extend(additional_docs)
            
            logger.info(f"æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:50]}...', ç»“æœæ•°é‡={len(documents)}")
            return documents[:k]
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def delete_documents(self, filter_dict: Dict[str, Any]) -> bool:
        """
        åˆ é™¤æ–‡æ¡£
        
        Args:
            filter_dict: åˆ é™¤æ¡ä»¶
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            where_clause = {}
            for key, value in filter_dict.items():
                where_clause[key] = {"$eq": value}
            
            self.collection.delete(where=where_clause)
            logger.info(f"åˆ é™¤æ–‡æ¡£æˆåŠŸ: {filter_dict}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            stats = {
                "collection_name": self.collection_name,
                "total_documents": self.collection.count(),
                "vector_db_path": settings.VECTOR_DB_PATH
            }
            
            # æ·»åŠ embeddingä¿¡æ¯
            if hasattr(self, 'embedding_manager'):
                method_info = self.embedding_manager.get_method_info()
                stats.update({
                    "embedding_method": method_info["method"],
                    "embeddings_dimension": method_info["dimension"],
                    "embedding_description": method_info["description"],
                    "is_free_embedding": method_info["is_free"],
                    "privacy_protected": method_info["privacy_protected"]
                })
            elif hasattr(self, 'embeddings') and self.embeddings is not None:
                stats.update({
                    "embedding_method": "openai",
                    "embeddings_dimension": 1536,
                    "embedding_description": "OpenAI Embeddings",
                    "is_free_embedding": False,
                    "privacy_protected": False
                })
            else:
                stats.update({
                    "embedding_method": "text-hash",
                    "embeddings_dimension": 384,
                    "embedding_description": "ç®€å•æ–‡æœ¬å“ˆå¸Œ",
                    "is_free_embedding": True,
                    "privacy_protected": True
                })
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def reset(self) -> bool:
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        try:
            self.client.delete_collection(self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "ä¸ªäººçŸ¥è¯†åº“å‘é‡å­˜å‚¨"}
            )
            logger.info("å‘é‡å­˜å‚¨é‡ç½®æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨é‡ç½®å¤±è´¥: {str(e)}")
            return False

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.bm25_cache = {}
    
    def hybrid_search(self, query: str, k: int = 5, 
                     vector_weight: float = 0.7, keyword_weight: float = 0.3,
                     filter_dict: Optional[Dict] = None) -> List[LangChainDocument]:
        """
        æ··åˆæ£€ç´¢ï¼šå‘é‡æœç´¢ + å…³é”®è¯æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ–‡æ¡£æ•°é‡
            vector_weight: å‘é‡æœç´¢æƒé‡
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡
            filter_dict: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æ··åˆæ£€ç´¢ç»“æœ
        """
        try:
            # å‘é‡è¯­ä¹‰æœç´¢
            vector_results = self.vector_store.search(
                query, 
                k=k*2, 
                filter_dict=filter_dict
            )
            
            # ç®€å•å…³é”®è¯æœç´¢ï¼ˆåŸºäºæ–‡ä»¶åå’Œå†…å®¹ï¼‰
            keyword_results = self._keyword_search(
                query, 
                k=k*2, 
                filter_dict=filter_dict
            )
            
            # èåˆç»“æœ
            combined_results = self._fusion_results(
                query,
                vector_results,
                keyword_results,
                vector_weight,
                keyword_weight
            )
            
            logger.info(f"æ··åˆæ£€ç´¢å®Œæˆ: æŸ¥è¯¢='{query[:30]}...', ç»“æœæ•°é‡={len(combined_results)}")
            return combined_results[:k]
            
        except Exception as e:
            logger.error(f"æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}")
            # é™çº§åˆ°å‘é‡æœç´¢
            return self.vector_store.search(query, k=k, filter_dict=filter_dict)
    
    def _keyword_search(self, query: str, k: int, filter_dict: Optional[Dict]) -> List[LangChainDocument]:
        """ç®€å•çš„å…³é”®è¯æœç´¢"""
        # TODO: å®ç°BM25æˆ–å…¶ä»–å…³é”®è¯æœç´¢ç®—æ³•
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
        return []
    
    def _fusion_results(self, query: str, vector_results: List[LangChainDocument], 
                       keyword_results: List[LangChainDocument], 
                       vector_weight: float, keyword_weight: float) -> List[LangChainDocument]:
        """èåˆæœç´¢ç»“æœ"""
        # ç®€å•èåˆç­–ç•¥ï¼šæŒ‰æƒé‡ç»„åˆç»“æœ
        all_results = []
        
        # æ·»åŠ å‘é‡æœç´¢ç»“æœ
        for doc in vector_results:
            doc.metadata = doc.metadata or {}
            doc.metadata['vector_score'] = doc.metadata.get('relevance_score', 0.5)
            doc.metadata['keyword_score'] = 0
            doc.metadata['combined_score'] = vector_weight * doc.metadata['vector_score']
            all_results.append(doc)
        
        # æ·»åŠ å…³é”®è¯æœç´¢ç»“æœ
        for doc in keyword_results:
            doc.metadata = doc.metadata or {}
            doc.metadata['vector_score'] = 0
            doc.metadata['keyword_score'] = 0.5  # ç®€åŒ–å¤„ç†
            doc.metadata['combined_score'] = keyword_weight * doc.metadata['keyword_score']
            all_results.append(doc)
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        all_results.sort(key=lambda x: x.metadata.get('combined_score', 0), reverse=True)
        
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£IDæˆ–å†…å®¹hashï¼‰
        seen = set()
        unique_results = []
        for doc in all_results:
            doc_hash = hash(doc.page_content)
            if doc_hash not in seen:
                seen.add(doc_hash)
                unique_results.append(doc)
        
        return unique_results
