# ä¸ªäººçŸ¥è¯†ç®¡ç†Agentç³»ç»Ÿ - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘æ–‡æ¡£

## æ–‡æ¡£ä¿¡æ¯
- **ç‰ˆæœ¬**: v1.0.0
- **åˆ›å»ºæ—¥æœŸ**: 2025-11-07
- **é€‚ç”¨äººå‘˜**: åç«¯å¼€å‘ã€ç®—æ³•å·¥ç¨‹å¸ˆã€æ–°å…¥èŒå¼€å‘è€…

---

## 1. æ ¸å¿ƒä¸šåŠ¡æ¦‚è¿°

æœ¬ç³»ç»ŸåŸºäº **RAG (Retrieval-Augmented Generation)** æŠ€æœ¯å®ç°çŸ¥è¯†é—®ç­”ï¼Œæ ¸å¿ƒä¸šåŠ¡é€»è¾‘åŒ…æ‹¬:
1. æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–
2. å‘é‡æ£€ç´¢ä¸ç›¸ä¼¼åº¦åŒ¹é…
3. ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ
4. å¤šæ–¹æ¡ˆEmbeddingé™çº§ç­–ç•¥

---

## 2. RAGé—®ç­”æµç¨‹è¯¦è§£

### 2.1 å®Œæ•´RAGæµç¨‹å›¾

```
ç”¨æˆ·æé—®
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. æŸ¥è¯¢é¢„å¤„ç†                                                â”‚
â”‚  â€¢ æ–‡æœ¬æ¸…æ´—                                                   â”‚
â”‚  â€¢ æŸ¥è¯¢ä¼˜åŒ– (å¯é€‰)                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. æŸ¥è¯¢Embeddingç”Ÿæˆ                                         â”‚
â”‚  â€¢ ä½¿ç”¨ä¸æ–‡æ¡£ç›¸åŒçš„embeddingæ–¹æ³•                              â”‚
â”‚  â€¢ ç”ŸæˆæŸ¥è¯¢å‘é‡ (384/1536ç»´)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢                                            â”‚
â”‚  â€¢ ChromaDB HNSWç´¢å¼•æœç´¢                                      â”‚
â”‚  â€¢ è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»                                           â”‚
â”‚  â€¢ è¿”å›Top-Kå€™é€‰æ–‡æ¡£ (k=5)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. ç›¸å…³æ€§è¯„åˆ†ä¸æ’åº                                          â”‚
â”‚  â€¢ è·ç¦»å½’ä¸€åŒ–                                                 â”‚
â”‚  â€¢ åŠ¨æ€é˜ˆå€¼è°ƒæ•´                                               â”‚
â”‚  â€¢ relevance_scoreè®¡ç®—                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. ä¸Šä¸‹æ–‡æ„å»º                                                â”‚
â”‚  â€¢ æ•´åˆæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ                                       â”‚
â”‚  â€¢ æ·»åŠ æ¥æºæ ‡è¯† [æ¥æº: æ–‡ä»¶å]                                â”‚
â”‚  â€¢ æ ¼å¼åŒ–ä¸ºLLMå¯è¯»æ ¼å¼                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                           â”‚
         â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æœ‰DeepSeek API     â”‚    â”‚ æ— API (é™çº§æ¨¡å¼)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 6a. LLMç”Ÿæˆ        â”‚    â”‚ 6b. ç®€åŒ–å›ç­”        â”‚
â”‚ â€¢ è°ƒç”¨DeepSeek API â”‚    â”‚ â€¢ è¿”å›æ£€ç´¢ç»“æœæ‘˜è¦  â”‚
â”‚ â€¢ æç¤ºè¯å·¥ç¨‹       â”‚    â”‚ â€¢ å±•ç¤ºç›¸å…³æ–‡æ¡£ç‰‡æ®µ  â”‚
â”‚ â€¢ å¼•ç”¨æ¥æº         â”‚    â”‚ â€¢ æ ‡æ³¨ç›¸å…³æ€§åˆ†æ•°    â”‚
â”‚ â€¢ ç½®ä¿¡åº¦è¯„ä¼°       â”‚    â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. ç»“æœè¿”å›                                                  â”‚
â”‚  â€¢ Markdownæ ¼å¼åŒ–                                             â”‚
â”‚  â€¢ æ·»åŠ æ¥æºå¼•ç”¨                                               â”‚
â”‚  â€¢ æ˜¾ç¤ºç½®ä¿¡åº¦/ç›¸å…³æ€§                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒä»£ç å®ç°

#### 2.2.1 chat_with_knowledge() ä¸»æµç¨‹
ä½ç½®: `src/api/gradio_app.py`

```python
def chat_with_knowledge(self, message: str, history: List[Dict[str, str]]) -> str:
    # 1. è¾“å…¥éªŒè¯
    if not message.strip():
        return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚"
    
    # 2. å‘é‡æ£€ç´¢
    retrieved_docs = self.vector_store.search(message, k=settings.TOP_K)
    
    if not retrieved_docs:
        return "æˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯..."
    
    # 3. æ™ºèƒ½æ¨¡å¼: ä½¿ç”¨RAGç”Ÿæˆå™¨
    if self.llm_enabled and self.rag_generator:
        result = self.rag_generator.generate_answer(
            query=message,
            documents=retrieved_docs,
            include_sources=True
        )
        # æ„å»ºç­”æ¡ˆ (å¸¦AIç”Ÿæˆå†…å®¹ã€ç½®ä¿¡åº¦ã€æ¥æº)
        answer = self._build_ai_answer(result)
        return answer
    
    # 4. ç®€åŒ–æ¨¡å¼: åŸºäºæ£€ç´¢ç»“æœçš„æ‘˜è¦
    answer = self._build_simple_answer(retrieved_docs, message)
    return answer
```

#### 2.2.2 RAGç”Ÿæˆå™¨æ ¸å¿ƒé€»è¾‘
ä½ç½®: `src/generation/llm_manager.py`

```python
def generate_answer(self, query: str, documents: List[LangChainDocument], 
                   include_sources: bool = True) -> Dict[str, Any]:
    # 1. æ„å»ºä¸Šä¸‹æ–‡
    context = self._build_context(documents, include_sources)
    
    # 2. æ„å»ºæç¤ºè¯
    messages = [
        SystemMessage(content=self.rag_system_prompt.format(
            context=context,
            question=query
        )),
        HumanMessage(content="è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚")
    ]
    
    # 3. è°ƒç”¨LLM
    response = self.chat_model.invoke(messages)
    answer = response.content
    
    # 4. ä¼°ç®—ç½®ä¿¡åº¦
    confidence = self._estimate_confidence(query, documents, answer)
    
    # 5. è¿”å›ç»“æœ
    return {
        "answer": answer,
        "confidence": confidence,
        "sources": self._extract_sources(documents),
        "metadata": {
            "query": query,
            "retrieved_docs": len(documents),
            "tokens_used": response.usage_metadata.get('total_tokens', 0)
        }
    }
```

---

## 3. Embeddingæ–¹æ¡ˆè¯¦è§£

### 3.1 å¤šæ–¹æ¡ˆæ¶æ„

ç³»ç»Ÿæ”¯æŒ4ç§Embeddingæ–¹æ¡ˆï¼ŒæŒ‰ä¼˜å…ˆçº§è‡ªåŠ¨é€‰æ‹©:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Embeddingæ–¹æ¡ˆé€‰æ‹©ç­–ç•¥                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1ï¸âƒ£ OpenAI text-embedding-3-small (æœ€é«˜è´¨é‡)              â”‚
â”‚     æ¡ä»¶: æœ‰OPENAI_API_KEY                                 â”‚
â”‚     ç»´åº¦: 1536                                             â”‚
â”‚     ç‰¹ç‚¹: äº‘ç«¯APIã€é«˜è´¨é‡ã€éœ€ä»˜è´¹                          â”‚
â”‚              â†“ (APIä¸å¯ç”¨)                                 â”‚
â”‚                                                            â”‚
â”‚  2ï¸âƒ£ Sentence Transformers (all-MiniLM-L6-v2)             â”‚
â”‚     æ¡ä»¶: å®‰è£…äº†sentence-transformersåº“                    â”‚
â”‚     ç»´åº¦: 384                                              â”‚
â”‚     ç‰¹ç‚¹: æœ¬åœ°è¿è¡Œã€å…è´¹ã€é«˜è´¨é‡                           â”‚
â”‚              â†“ (åº“æœªå®‰è£…/ç½‘ç»œé—®é¢˜)                         â”‚
â”‚                                                            â”‚
â”‚  3ï¸âƒ£ æ–‡æœ¬å“ˆå¸Œ (Text Hash)                                  â”‚
â”‚     æ¡ä»¶: æ— æ¡ä»¶ (ä¿åº•æ–¹æ¡ˆ)                                â”‚
â”‚     ç»´åº¦: 384                                              â”‚
â”‚     ç‰¹ç‚¹: é›¶ä¾èµ–ã€æå¿«ã€ä¸­ç­‰è´¨é‡                           â”‚
â”‚                                                            â”‚
â”‚  4ï¸âƒ£ TF-IDFè¯è¢‹ (å¯é€‰)                                     â”‚
â”‚     æ¡ä»¶: å®‰è£…äº†scikit-learn                               â”‚
â”‚     ç»´åº¦: 1000                                             â”‚
â”‚     ç‰¹ç‚¹: å…³é”®è¯ä¼˜åŒ–ã€é€‚åˆå…³é”®è¯æœç´¢                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 å„æ–¹æ¡ˆå®ç°ç»†èŠ‚

#### 3.2.1 Sentence Transformers (æ¨è)
ä½ç½®: `src/storage/embedding_manager.py`

**å®ç°**:
```python
def _try_sentence_transformers(self) -> bool:
    from sentence_transformers import SentenceTransformer
    
    model_name = "all-MiniLM-L6-v2"
    self.model = SentenceTransformer(model_name)
    self.method = "sentence-transformers"
    self.embedding_dim = 384
    return True

def _embed_sentence_transformers(self, texts: List[str]) -> List[List[float]]:
    return self.model.encode(texts, show_progress_bar=False).tolist()
```

**ç‰¹ç‚¹**:
- âœ… è´¨é‡é«˜: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚
- âœ… é€Ÿåº¦å¿«: æœ¬åœ°GPUåŠ é€Ÿ (å¦‚æœæœ‰CUDA)
- âœ… ç¦»çº¿å¯ç”¨: æ¨¡å‹ä¸‹è½½åå¯ç¦»çº¿ä½¿ç”¨
- âš ï¸ é¦–æ¬¡éœ€è”ç½‘: ä¸‹è½½æ¨¡å‹æ–‡ä»¶ (~90MB)

#### 3.2.2 æ–‡æœ¬å“ˆå¸Œ (ä¿åº•æ–¹æ¡ˆ)
ä½ç½®: `src/storage/embedding_manager.py`

**å®ç°**:
```python
def _embed_text_hash(self, texts: List[str]) -> List[List[float]]:
    embeddings = []
    for text in texts:
        hash_features = []
        for i in range(self.embedding_dim):  # 384ç»´
            # ä½¿ç”¨ä¸åŒç§å­ç”Ÿæˆå“ˆå¸Œ
            hash_obj = hashlib.md5(f"{text}_{i}".encode())
            hash_value = int(hash_obj.hexdigest(), 16) % 1000
            hash_features.append(hash_value / 1000.0)
        embeddings.append(hash_features)
    return embeddings
```

**ç‰¹ç‚¹**:
- âœ… é›¶ä¾èµ–: åªéœ€Pythonæ ‡å‡†åº“
- âœ… æå¿«: çº¯Pythonå®ç°ï¼Œæ— æ¨¡å‹åŠ è½½
- âœ… ç¨³å®šæ€§: ç›¸åŒæ–‡æœ¬å§‹ç»ˆç”Ÿæˆç›¸åŒå‘é‡
- âš ï¸ è¯­ä¹‰ç†è§£å¼±: æ— æ³•æ•æ‰æ·±å±‚è¯­ä¹‰

#### 3.2.3 TF-IDFè¯è¢‹
ä½ç½®: `src/storage/embedding_manager.py`

**å®ç°**:
```python
def _embed_bow_tfidf(self, texts: List[str]) -> List[List[float]]:
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    if not hasattr(self, '_tfidf_vectorizer'):
        self._tfidf_vectorizer = TfidfVectorizer(
            max_features=self.embedding_dim,  # 1000ç»´
            stop_words='english',
            lowercase=True
        )
    
    tfidf_matrix = self._tfidf_vectorizer.fit_transform(texts)
    return tfidf_matrix.toarray().tolist()
```

**ç‰¹ç‚¹**:
- âœ… å…³é”®è¯æ•æ„Ÿ: é€‚åˆå…³é”®è¯æœç´¢
- âœ… å¯è§£é‡Šæ€§å¼º: æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªè¯
- âš ï¸ éœ€è¦è®­ç»ƒ: é¦–æ¬¡éœ€è¦åœ¨è¯­æ–™ä¸Šfit
- âš ï¸ æ— æ³•å¤„ç†æœªè§è¯: OOVé—®é¢˜

---

## 4. æ£€ç´¢ç®—æ³•è¯¦è§£

### 4.1 å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢

#### 4.1.1 HNSWç®—æ³•åŸç†
ChromaDBä½¿ç”¨ **HNSW (Hierarchical Navigable Small World)** ç´¢å¼•:

```
å¤šå±‚å›¾ç»“æ„:
Layer 2: [Node A] â”€â”€â”€â”€ [Node B]
           â”‚              â”‚
Layer 1: [A]â”€â”€[C]â”€â”€[D]â”€â”€[B]â”€â”€[E]
           â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”‚
Layer 0: [A][X][C][Y][D][Z][B][W][E]
         â””â”€ æ‰€æœ‰èŠ‚ç‚¹ â”€â”˜

æŸ¥è¯¢è¿‡ç¨‹:
1. ä»æœ€é«˜å±‚å…¥å£èŠ‚ç‚¹å¼€å§‹
2. è´ªå¿ƒæœç´¢æ‰¾åˆ°è¯¥å±‚æœ€è¿‘é‚»
3. ä¸‹é™åˆ°ä¸‹ä¸€å±‚
4. é‡å¤ç›´åˆ°Layer 0
5. è¿”å›Top-Kæœ€è¿‘é‚»
```

**æ—¶é—´å¤æ‚åº¦**: O(log N)

#### 4.1.2 ç›¸ä¼¼åº¦è®¡ç®—
ä½ç½®: `src/storage/vector_store_simple.py`

**æ¬§å‡ é‡Œå¾—è·ç¦»**:
```python
# ChromaDBå†…éƒ¨è®¡ç®—
distance = sqrt(sum((query[i] - doc[i])^2 for i in range(dim)))
```

**ç›¸å…³æ€§åˆ†æ•°è½¬æ¢**:
```python
def calculate_relevance_score(distance, min_dist, max_dist):
    # 1. å½’ä¸€åŒ–åˆ° [0, 1]
    if max_dist > min_dist:
        relative_distance = (distance - min_dist) / (max_dist - min_dist)
        relevance_score = 1.0 - relative_distance
    else:
        relevance_score = 0.5
    
    # 2. åŠ¨æ€é˜ˆå€¼è°ƒæ•´
    if distance > 2.0:  # è·ç¦»å¾ˆè¿œ
        relevance_score = max(0.0, min(0.3, relevance_score))
    elif distance > 1.5:  # è·ç¦»è¾ƒè¿œ
        relevance_score = max(0.1, min(0.5, relevance_score))
    elif distance < 0.3:  # éå¸¸ç›¸ä¼¼
        relevance_score = max(0.7, min(1.0, relevance_score))
    else:  # ä¸­ç­‰è·ç¦»
        relevance_score = max(0.2, min(0.8, relevance_score))
    
    return relevance_score
```

### 4.2 æ··åˆæ£€ç´¢ (Hybrid Retrieval)

#### 4.2.1 æ··åˆæ£€ç´¢ç­–ç•¥
ä½ç½®: `src/storage/vector_store_simple.py`

```python
def hybrid_search(self, query: str, k: int = 5, 
                 vector_weight: float = 0.7, 
                 keyword_weight: float = 0.3) -> List[LangChainDocument]:
    # 1. å‘é‡è¯­ä¹‰æœç´¢
    vector_results = self.vector_store.search(query, k=k*2)
    
    # 2. å…³é”®è¯æœç´¢ (TODO: å½“å‰æœªå®Œå…¨å®ç°)
    keyword_results = self._keyword_search(query, k=k*2)
    
    # 3. èåˆç»“æœ
    combined_results = self._fusion_results(
        vector_results,
        keyword_results,
        vector_weight,
        keyword_weight
    )
    
    return combined_results[:k]
```

#### 4.2.2 ç»“æœèåˆç®—æ³•
```python
def _fusion_results(self, vector_results, keyword_results, 
                   vector_weight, keyword_weight):
    all_results = []
    
    # æ·»åŠ å‘é‡æœç´¢ç»“æœ
    for doc in vector_results:
        doc.metadata['vector_score'] = doc.metadata.get('relevance_score', 0.5)
        doc.metadata['keyword_score'] = 0
        doc.metadata['combined_score'] = vector_weight * doc.metadata['vector_score']
        all_results.append(doc)
    
    # æ·»åŠ å…³é”®è¯æœç´¢ç»“æœ
    for doc in keyword_results:
        doc.metadata['vector_score'] = 0
        doc.metadata['keyword_score'] = 0.5
        doc.metadata['combined_score'] = keyword_weight * doc.metadata['keyword_score']
        all_results.append(doc)
    
    # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
    all_results.sort(key=lambda x: x.metadata.get('combined_score', 0), reverse=True)
    
    # å»é‡
    unique_results = remove_duplicates(all_results)
    
    return unique_results
```

**æ³¨æ„**: å½“å‰ç‰ˆæœ¬å…³é”®è¯æ£€ç´¢éƒ¨åˆ†è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸»è¦ä¾èµ–å‘é‡æ£€ç´¢ã€‚

---

## 5. æ–‡æ¡£å¤„ç†é€»è¾‘

### 5.1 æ–‡æ¡£åŠ è½½ä¸åˆ†å—

#### 5.1.1 æ–‡æœ¬åˆ†å—ç®—æ³•
ä½ç½®: `src/ingestion/document_loader_simple.py`

**åˆ†å—ç­–ç•¥**:
```python
def simple_text_splitter(text: str, chunk_size: int = 1000, 
                        chunk_overlap: int = 200) -> List[str]:
    # 1. æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°å—
        if len(current_chunk) + len(paragraph) + 2 > chunk_size:
            if current_chunk:
                chunks.append(current_chunk)
                # 3. ä¿æŒé‡å 
                overlap_start = max(0, len(current_chunk) - chunk_overlap)
                current_chunk = current_chunk[overlap_start:] + " " + paragraph
            else:
                current_chunk = paragraph
        else:
            current_chunk += "\n\n" + paragraph
    
    # 4. è¿›ä¸€æ­¥åˆ†å‰²å¤§å— (æŒ‰å¥å­)
    final_chunks = further_split_by_sentences(chunks, chunk_size, chunk_overlap)
    
    return final_chunks
```

**åˆ†å—å‚æ•°**:
- `chunk_size`: 1000å­—ç¬¦
- `chunk_overlap`: 200å­—ç¬¦
- **ç›®çš„**: ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œé¿å…æˆªæ–­å¥å­

#### 5.1.2 æ–‡æ¡£å…ƒæ•°æ®æå–
```python
def load_file(self, file_path: str) -> List[LangChainDocument]:
    # 1. æå–æ–‡æœ¬å†…å®¹
    content = self._extract_text(file_path)
    
    # 2. åˆ†å—
    chunks = simple_text_splitter(content, self.chunk_size, self.chunk_overlap)
    
    # 3. è½¬æ¢ä¸ºLangChainæ–‡æ¡£
    documents = []
    for i, chunk in enumerate(chunks):
        doc = LangChainDocument(
            page_content=chunk,
            metadata={
                "source": str(file_path),
                "filename": file_path.name,
                "chunk_id": i,
                "total_chunks": len(chunks),
                "file_type": file_path.suffix,
                "file_size": file_path.stat().st_size / (1024*1024),
                "content_hash": str(hash(chunk))
            }
        )
        documents.append(doc)
    
    return documents
```

### 5.2 æ–‡æ¡£åˆ†ç±»ç®—æ³•

#### 5.2.1 åŸºäºè§„åˆ™çš„åˆ†ç±»
ä½ç½®: `src/ingestion/document_loader_simple.py`

```python
def classify_document(self, document: LangChainDocument) -> Dict[str, Any]:
    content = document.page_content.lower()
    
    # åˆ†ç±»è§„åˆ™
    category_rules = {
        "å·¥ä½œ": ["é¡¹ç›®", "ä¼šè®®", "å·¥ä½œ", "ä»»åŠ¡", "è®¡åˆ’", "æŠ¥å‘Š"],
        "å­¦ä¹ ": ["å­¦ä¹ ", "æ•™ç¨‹", "è¯¾ç¨‹", "ç¬”è®°", "çŸ¥è¯†", "æŠ€èƒ½"],
        "ä¸ªäºº": ["æ—¥è®°", "æƒ³æ³•", "æ„Ÿæ‚Ÿ", "ç”Ÿæ´»", "å®¶åº­", "ä¸ªäºº"],
        "å‚è€ƒ": ["å‚è€ƒ", "æ–‡æ¡£", "æ‰‹å†Œ", "æŒ‡å—", "è§„èŒƒ", "æ ‡å‡†"],
        "ç ”ç©¶": ["ç ”ç©¶", "åˆ†æ", "å®éªŒ", "æ•°æ®", "ç»“è®º", "å‘ç°"],
        "æƒ³æ³•": ["æƒ³æ³•", "åˆ›æ„", "åˆ›æ–°", "è®¾è®¡", "æ¦‚å¿µ", "æ€è·¯"]
    }
    
    # å…³é”®è¯åŒ¹é…è®¡åˆ†
    scores = {}
    for category, keywords in category_rules.items():
        score = sum(1 for keyword in keywords if keyword in content)
        scores[category] = score
    
    # é€‰æ‹©æœ€é«˜åˆ†ç±»åˆ«
    category = max(scores, key=scores.get) if scores else "å‚è€ƒ"
    if scores[category] == 0:
        category = "å‚è€ƒ"  # é»˜è®¤åˆ†ç±»
    
    # ä¼˜å…ˆçº§åˆ¤æ–­
    if "é‡è¦" in content or "ç´§æ€¥" in content:
        priority = "é«˜"
    elif len(content) > 2000:
        priority = "ä¸­"
    else:
        priority = "ä½"
    
    return {
        "category": category,
        "priority": priority,
        "summary": content[:100] + "...",
        "tags": ",".join(self._extract_keywords(content)),
        "confidence": scores[category] / max(len(content.split()) / 100, 1)
    }
```

#### 5.2.2 å…³é”®è¯æå–
```python
def _extract_keywords(self, content: str) -> List[str]:
    common_words = {"çš„", "äº†", "æ˜¯", "åœ¨", "æœ‰", "å’Œ"}
    
    words = content.split()
    word_freq = {}
    
    for word in words:
        word = word.strip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''()ï¼ˆï¼‰')
        if len(word) > 1 and word not in common_words:
            word_freq[word] = word_freq.get(word, 0) + 1
    
    # è¿”å›é¢‘ç‡æœ€é«˜çš„5ä¸ªè¯
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
    return [word for word, freq in top_words]
```

---

## 6. æç¤ºè¯å·¥ç¨‹

### 6.1 RAGç³»ç»Ÿæç¤ºè¯

ä½ç½®: `src/generation/llm_manager.py`

```python
rag_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†ç®¡ç†åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. ä»…ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å›ç­”é—®é¢˜
2. æ¯ä¸ªäº‹å®é™ˆè¿°éƒ½è¦å¼•ç”¨æ¥æºï¼š[æ¥æºï¼šæ–‡ä»¶å]
3. å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç­”æ¡ˆï¼Œæ˜ç¡®è¯´æ˜"æˆ‘åœ¨ç°æœ‰æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯"
4. ä½¿ç”¨Markdownæ ¼å¼ç»„ç»‡ç­”æ¡ˆ
5. ä¿æŒå®¢è§‚ã€å‡†ç¡®ã€æœ‰å¸®åŠ©
6. ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡å›ç­”

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šè¿°è§„åˆ™å’Œä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š"""
```

**è®¾è®¡åŸåˆ™**:
- âœ… æ˜ç¡®è§’è‰²å®šä½
- âœ… æ¸…æ™°çš„è§„åˆ™çº¦æŸ
- âœ… å¼ºåˆ¶æ¥æºå¼•ç”¨
- âœ… é¿å…å¹»è§‰ (Hallucination)

### 6.2 ä¸Šä¸‹æ–‡æ„å»º

```python
def _build_context(self, documents: List[LangChainDocument], 
                  include_sources: bool = True) -> str:
    context_parts = []
    
    for i, doc in enumerate(documents, 1):
        filename = doc.metadata.get('filename', 'æœªçŸ¥æ–‡ä»¶')
        content = doc.page_content
        
        if include_sources:
            context_parts.append(f"[æ–‡æ¡£ {i} - æ¥æº: {filename}]\n{content}\n")
        else:
            context_parts.append(f"{content}\n")
    
    return "\n---\n".join(context_parts)
```

**ä¸Šä¸‹æ–‡ç¤ºä¾‹**:
```
[æ–‡æ¡£ 1 - æ¥æº: python_notes.md]
Pythonè£…é¥°å™¨æ˜¯ä¸€ç§è®¾è®¡æ¨¡å¼ï¼Œå…è®¸åœ¨ä¸ä¿®æ”¹åŸå‡½æ•°ä»£ç çš„æƒ…å†µä¸‹...

---

[æ–‡æ¡£ 2 - æ¥æº: python_notes.md]
è£…é¥°å™¨çš„åŸºæœ¬è¯­æ³•æ˜¯ä½¿ç”¨@ç¬¦å·ï¼Œä¾‹å¦‚ï¼š@decorator...

---

[æ–‡æ¡£ 3 - æ¥æº: advanced_python.pdf]
è£…é¥°å™¨å¯ä»¥æ¥å—å‚æ•°ï¼Œå®ç°æ›´çµæ´»çš„åŠŸèƒ½æ‰©å±•...
```

---

## 7. ç½®ä¿¡åº¦è¯„ä¼°

### 7.1 ç½®ä¿¡åº¦è®¡ç®—é€»è¾‘

ä½ç½®: `src/generation/llm_manager.py`

```python
def _estimate_confidence(self, query: str, documents: List[LangChainDocument], 
                        answer: str) -> float:
    confidence = 0.0
    
    # 1. åŸºäºæ–‡æ¡£ç›¸å…³æ€§
    relevance_scores = [
        doc.metadata.get('relevance_score', 0.5) 
        for doc in documents
    ]
    avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.5
    confidence += avg_relevance * 0.4  # 40%æƒé‡
    
    # 2. åŸºäºæ–‡æ¡£æ•°é‡
    doc_count_score = min(len(documents) / 5.0, 1.0)
    confidence += doc_count_score * 0.2  # 20%æƒé‡
    
    # 3. åŸºäºç­”æ¡ˆé•¿åº¦
    answer_length_score = min(len(answer) / 500, 1.0)
    confidence += answer_length_score * 0.2  # 20%æƒé‡
    
    # 4. åŸºäºå…³é”®è¯åŒ¹é…
    query_keywords = set(query.lower().split())
    answer_keywords = set(answer.lower().split())
    keyword_overlap = len(query_keywords & answer_keywords) / len(query_keywords) if query_keywords else 0
    confidence += keyword_overlap * 0.2  # 20%æƒé‡
    
    return min(confidence, 1.0)
```

**ç½®ä¿¡åº¦ç­‰çº§**:
- **0.8 - 1.0**: é«˜ç½®ä¿¡åº¦ (æ–‡æ¡£é«˜åº¦ç›¸å…³ï¼Œç­”æ¡ˆå……åˆ†)
- **0.5 - 0.8**: ä¸­ç­‰ç½®ä¿¡åº¦ (æ–‡æ¡£ç›¸å…³ï¼Œä½†å¯èƒ½ä¸å®Œæ•´)
- **0.0 - 0.5**: ä½ç½®ä¿¡åº¦ (æ–‡æ¡£ç›¸å…³æ€§ä½ï¼Œç­”æ¡ˆä¸ç¡®å®š)

---

## 8. é™çº§ç­–ç•¥è¯¦è§£

### 8.1 Embeddingé™çº§é“¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Embeddingç”Ÿæˆé™çº§æµç¨‹                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. å°è¯• OpenAI API                            â”‚
â”‚     if settings.OPENAI_API_KEY:               â”‚
â”‚         return OpenAIEmbeddings(...)          â”‚
â”‚                â†“ (å¤±è´¥)                        â”‚
â”‚                                               â”‚
â”‚  2. å°è¯• Sentence Transformers                â”‚
â”‚     try:                                      â”‚
â”‚         import sentence_transformers          â”‚
â”‚         return SentenceTransformer(...)       â”‚
â”‚                â†“ (å¤±è´¥)                        â”‚
â”‚                                               â”‚
â”‚  3. å›é€€åˆ°æ–‡æœ¬å“ˆå¸Œ                             â”‚
â”‚     return TextHashEmbedding(...)             â”‚
â”‚     (ä¿åº•æ–¹æ¡ˆï¼Œæ°¸ä¸å¤±è´¥)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®ç°ä»£ç **:
```python
def _initialize_embeddings(self):
    # ä¼˜å…ˆçº§1: OpenAI
    if settings.OPENAI_API_KEY and OPENAI_AVAILABLE:
        try:
            self.embeddings = OpenAIEmbeddings(...)
            logger.info("âœ… ä½¿ç”¨OpenAI Embeddings")
            return
        except Exception as e:
            logger.warning(f"OpenAIåˆå§‹åŒ–å¤±è´¥: {e}")
    
    # ä¼˜å…ˆçº§2: Sentence Transformers
    if EMBEDDING_MANAGER_AVAILABLE:
        try:
            optimal_method = settings.get_optimal_embedding_method()
            self.embedding_manager = EmbeddingManager(optimal_method)
            logger.info(f"âœ… ä½¿ç”¨{optimal_method}")
            return
        except Exception as e:
            logger.warning(f"Embeddingç®¡ç†å™¨å¤±è´¥: {e}")
    
    # ä¼˜å…ˆçº§3: æ–‡æœ¬å“ˆå¸Œ (ä¿åº•)
    logger.info("ğŸ”„ ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œembedding")
    self.embeddings = None  # è§¦å‘æ–‡æœ¬å“ˆå¸Œé€»è¾‘
```

### 8.2 LLMç”Ÿæˆé™çº§é“¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LLMç”Ÿæˆé™çº§æµç¨‹                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. æœ‰DeepSeek APIå¯†é’¥                         â”‚
â”‚     if settings.DEEPSEEK_API_KEY:             â”‚
â”‚         return RAGGenerator.generate_answer() â”‚
â”‚                â†“ (æ— APIå¯†é’¥)                   â”‚
â”‚                                               â”‚
â”‚  2. ç®€åŒ–å›ç­”æ¨¡å¼                               â”‚
â”‚     return build_simple_answer()              â”‚
â”‚     â€¢ å±•ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ‘˜è¦                     â”‚
â”‚     â€¢ æ ‡æ³¨ç›¸å…³æ€§åˆ†æ•°                           â”‚
â”‚     â€¢ æä¾›æ–‡æ¡£æ¥æºé“¾æ¥                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 9.1 æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
# âœ… æ‰¹é‡ç”Ÿæˆembedding
embeddings = embedding_manager.embed_documents(texts)  # ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ª

# âŒ é¿å…å¾ªç¯è°ƒç”¨
for text in texts:
    embedding = embedding_manager.embed_query(text)  # æ•ˆç‡ä½
```

### 9.2 ç¼“å­˜æœºåˆ¶ (å¾…å®ç°)
```python
# TODO: å®ç°embeddingç¼“å­˜
embedding_cache = {}

def get_embedding_with_cache(text: str):
    if text in embedding_cache:
        return embedding_cache[text]
    
    embedding = embedding_manager.embed_query(text)
    embedding_cache[text] = embedding
    return embedding
```

### 9.3 Top-Ké™åˆ¶
```python
# é»˜è®¤è¿”å›5ä¸ªç»“æœï¼Œé¿å…è¿‡åº¦æ£€ç´¢
TOP_K = 5
results = vector_store.search(query, k=TOP_K)
```

---

## 10. æœªå®ç°åŠŸèƒ½æ¸…å•

### 10.1 å…³é”®è¯æ£€ç´¢ (BM25)
ä½ç½®: `src/storage/vector_store_simple.py:461`

```python
def _keyword_search(self, query: str, k: int, filter_dict: Optional[Dict]) -> List[LangChainDocument]:
    """ç®€å•çš„å…³é”®è¯æœç´¢"""
    # TODO: å®ç°BM25æˆ–å…¶ä»–å…³é”®è¯æœç´¢ç®—æ³•
    # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
    return []
```

**å»ºè®®å®ç°**:
```python
from rank_bm25 import BM25Okapi

def _keyword_search(self, query: str, k: int, filter_dict: Optional[Dict]):
    # 1. è·å–æ‰€æœ‰æ–‡æ¡£
    all_docs = self.vector_store.collection.get()
    
    # 2. æ„å»ºBM25ç´¢å¼•
    tokenized_docs = [doc.split() for doc in all_docs['documents']]
    bm25 = BM25Okapi(tokenized_docs)
    
    # 3. æœç´¢
    query_tokens = query.split()
    scores = bm25.get_scores(query_tokens)
    
    # 4. è¿”å›Top-K
    top_indices = np.argsort(scores)[::-1][:k]
    # ... æ„å»ºè¿”å›ç»“æœ
```

### 10.2 é«˜çº§æ–‡æ¡£åˆ†ç±»
å½“å‰ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Œå¯ä»¥å‡çº§ä¸ºLLMåˆ†ç±»:

```python
# å½“å‰: åŸºäºè§„åˆ™
category = classify_by_keywords(content)

# å»ºè®®: ä½¿ç”¨LLM
if llm_manager.chat_model:
    category = llm_manager.classify_document(content)
```

---

## 11. è°ƒè¯•ä¸ç›‘æ§

### 11.1 æ—¥å¿—è®°å½•
```python
# å…³é”®æ­¥éª¤è®°å½•æ—¥å¿—
logger.info(f"æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:50]}...', ç»“æœæ•°é‡={len(documents)}")
logger.warning(f"ä¸»æ–¹æ¡ˆå¤±è´¥: {e}, é™çº§åˆ°å¤‡ç”¨æ–¹æ¡ˆ")
logger.error(f"æ“ä½œå¤±è´¥: {str(e)}")
```

### 11.2 æ€§èƒ½ç›‘æ§
```python
import time

start_time = time.time()
results = vector_store.search(query, k=5)
elapsed_time = time.time() - start_time
logger.info(f"æ£€ç´¢è€—æ—¶: {elapsed_time:.3f}ç§’")
```

---

## 12. æœ€ä½³å®è·µ

### 12.1 RAGä¼˜åŒ–å»ºè®®
- âœ… åˆç†è®¾ç½®chunk_size (æ¨è1000)
- âœ… ä¿æŒchunk_overlap (æ¨è200)
- âœ… Top-Kä¸å®œè¿‡å¤§ (æ¨è5)
- âœ… æç¤ºè¯æ¸…æ™°æ˜ç¡®
- âœ… å¼ºåˆ¶æ¥æºå¼•ç”¨

### 12.2 Embeddingé€‰æ‹©
- ğŸ’° **æ— é¢„ç®—**: æ–‡æœ¬å“ˆå¸Œ
- ğŸ¯ **å¹³è¡¡**: Sentence Transformers (æ¨è)
- ğŸ† **é«˜è´¨é‡**: OpenAI Embeddings

### 12.3 é”™è¯¯å¤„ç†
- âœ… æ¯ä¸ªå…³é”®æ“ä½œéƒ½æœ‰try-except
- âœ… é™çº§ç­–ç•¥å®Œå–„
- âœ… ç”¨æˆ·å‹å¥½çš„é”™è¯¯æç¤º

---

## 13. å‚è€ƒèµ„æ–™

- [RAGè®ºæ–‡: Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)
- [HNSWç®—æ³•](https://arxiv.org/abs/1603.09320)
- [BM25ç®—æ³•è¯¦è§£](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html)
- [æç¤ºè¯å·¥ç¨‹æŒ‡å—](https://www.promptingguide.ai/)

---

**æ–‡æ¡£ç»´æŠ¤**: æ ¸å¿ƒé€»è¾‘å˜æ›´æ—¶è¯·åŠæ—¶æ›´æ–°æœ¬æ–‡æ¡£
